{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3.3 Transformers\n",
    "\n",
    "\n",
    "\n",
    " In this notebook, we will explore the Transformer architecture, which has become the foundation for modern NLP models. We will cover the core mechanisms, implement simple experiments, and work with pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention is All You Need\" (Vaswani et al., 2017), replaced recurrent architectures with a mechanism based entirely on attention. This allows for better parallelization and the ability to model long-range dependencies in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Attention Mechanism\n",
    " At the core of transformers is the attention mechanism. Unlike previous architectures that processed sequences sequentially, attention allows the model to focus on different parts of the input simultaneously.\n",
    "\n",
    " The key idea is that for each position in a sequence, we compute how much attention to pay to every other position. This is done through three learned representations:\n",
    "\n",
    "\n",
    " - **Queries (Q)**: What we're looking for\n",
    " - **Keys (K)**: What each position offers\n",
    " - **Values (V)**: The actual information at each position\n",
    "\n",
    "\n",
    "\n",
    " <!-- ![Attention Mechanism](fig_showing_attention) -->\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/attention_example.svg\" alt=\"attn_nn\" style=\"max-width:50%;\">\n",
    "</p>\n",
    "(img credits from uvadlc course)\n",
    "\n",
    "\n",
    " The attention score between a query and all keys is computed as:\n",
    "\n",
    "\n",
    "\n",
    " $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "\n",
    " where $d_k$ is the dimension of the keys. The division by $\\sqrt{d_k}$ prevents the dot products from growing too large, which would push the softmax into regions with very small gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multi-Head Attention (MHA)\n",
    "\n",
    "\n",
    "\n",
    " Rather than computing attention once, Multi-Head Attention runs multiple attention operations in parallel, each with different learned linear projections. This allows the model to attend to information from different representation subspaces.\n",
    "\n",
    "\n",
    "\n",
    " <!-- ![Multi-Head Attention](fig_showing_mha) -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/1*jsR691rbi1-LmYTBHzjDMA.png\" alt=\"fig_showing_mha\" style=\"max-width:15%;\">\n",
    "</p>\n",
    "\n",
    "\n",
    " Each \"head\" learns to focus on different aspects of the relationships between tokens. For example, one head might learn syntactic dependencies while another learns semantic relationships.\n",
    "\n",
    "\n",
    "\n",
    " The outputs from all heads are concatenated and linearly transformed:\n",
    "\n",
    "\n",
    "\n",
    " $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "\n",
    "\n",
    " where each head is computed as:\n",
    "\n",
    "\n",
    "\n",
    " $$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Encoder-Decoder Architecture\n",
    " The original Transformer uses an encoder-decoder structure:\n",
    "\n",
    " <!-- ![Encoder-Decoder Architecture](fig_showing_enc_dec) -->\n",
    " <p align=\"center\">\n",
    "    <img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\" alt=\"fig_showing_mha\" style=\"max-width:50%; background-color: white;\">\n",
    "</p>\n",
    "\n",
    " - **Encoder**: Processes the input sequence and builds a representation\n",
    " - **Decoder**: Generates the output sequence, attending to both its own previous outputs and the encoder's representations\n",
    "\n",
    " Modern models often use only the encoder (BERT) or only the decoder (GPT), depending on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore warning messages for cleaner output of the website\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toy Experiment: Next Token Prediction\n",
    "\n",
    "\n",
    "\n",
    " We'll implement a simple experiment to understand how transformers learn sequence patterns. We'll create a toy dataset and compare transformer models of different depths with n-gram baseline statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Toy Dataset\n",
    "\n",
    "\n",
    "\n",
    " We'll create a simple artificial language with predictable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")  ## Subset for faster training\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "len_data = 1000\n",
    "train_data = dataset.select(range(len_data))[\"text\"]\n",
    "test_data = dataset.select(range(len_data, len_data*2))[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_num(c):\n",
    "    \"\"\" Our mapping a mapping function from character to number\n",
    "         In this case: a=0, b=1, ..., z=25, space=26\n",
    "    \"\"\"\n",
    "    if c == ' ':\n",
    "        return 26\n",
    "    return ord(c) - ord('a')\n",
    "\n",
    "def process_texts(texts, N, char_to_num_fn):\n",
    "    \"\"\"\n",
    "    Processes a list of texts by lowercasing, removing non-alphabetic characters, truncating to a fixed length, and mapping each character to a numeric value. \n",
    "    Returns a list of lists of mapped numbers for valid texts.\n",
    "    \"\"\"\n",
    "    processed_nums = []\n",
    "    for text in texts:\n",
    "        text = text.lower() ## Convert to lowercase        \n",
    "        text = re.sub(r'[^a-z ]', '', text)  ## Keep only a-z and space \n",
    "        \n",
    "\n",
    "        ## if size is less than N, skip\n",
    "        if len(text) < N:\n",
    "            continue\n",
    "\n",
    "        text = text[:N] ## Keep only the first N characters\n",
    "\n",
    "        ## Convert each char to number using mapping function\n",
    "        nums = [char_to_num_fn(c) for c in text]\n",
    "        if any(n > 26 or n < 0 for n in nums):\n",
    "            raise ValueError(f\"Character mapping returned invalid number: {nums} - text: {text}\")\n",
    "        \n",
    "        processed_nums.append(nums)\n",
    "    return processed_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "train_data = process_texts(train_data, N, char_to_num)\n",
    "test_data = process_texts(test_data, N, char_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 N-gram Baseline\n",
    "\n",
    "\n",
    "\n",
    " Before using transformers, let's establish a baseline using n-gram statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(data, n=3):\n",
    "    \"\"\"Build n-gram model from data - a \"\"\"\n",
    "    ngram_counts = {}\n",
    "    \n",
    "    for seq in data:\n",
    "        for i in range(len(seq) - n):\n",
    "            context = tuple(seq[i:i+n])\n",
    "            next_token = seq[i+n]\n",
    "            if context not in ngram_counts:\n",
    "                ngram_counts[context] = Counter()\n",
    "            ngram_counts[context][next_token] += 1\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "def ngram_predict(ngram_model, context):\n",
    "    \"\"\"Predict next token given context\"\"\"\n",
    "    context = tuple(context)\n",
    "    if context in ngram_model:\n",
    "        return ngram_model[context].most_common(1)[0][0]\n",
    "    return np.random.randint(0, 10)  ## Random if context not seen\n",
    "\n",
    "def evaluate_ngram(ngram_model, test_data, n=3, start_eval=3):\n",
    "    \"\"\"Evaluate n-gram model accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for seq in test_data:\n",
    "        for i in range(start_eval, len(seq)):\n",
    "            context = seq[i-n:i]\n",
    "            pred = ngram_predict(ngram_model, context)\n",
    "            if pred == seq[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram accuracy: 42.0 %\n",
      "Trigram accuracy: 50.7 %\n"
     ]
    }
   ],
   "source": [
    "## Build and evaluate n-gram models\n",
    "trigram_model = build_ngram_model(train_data, n=3)\n",
    "trigram_acc = evaluate_ngram(trigram_model, test_data, n=3, start_eval=3)\n",
    "\n",
    "bigram_model = build_ngram_model(train_data, n=2)\n",
    "bigram_acc = evaluate_ngram(bigram_model, test_data, n=2, start_eval=3)\n",
    "\n",
    "print(f\"Bigram accuracy: {bigram_acc*100:.1f} %\")\n",
    "print(f\"Trigram accuracy: {trigram_acc*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Simple Transformer Model\n",
    "\n",
    "\n",
    "\n",
    " Now let's create transformer models with different numbers of layers using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" \n",
    "    Positional Encoding module for Transformer models \n",
    "        -> We don't learn this but keep it fixed. It ensures that the model can distinguish token positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  ## Shape: (1, max_len, d_model) for batch_first\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" This function adds positional encoding to the input tensor x and retursns it.\"\"\"\n",
    "        ## x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]  ## Use seq_len (dimension 1)!\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=1, dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=0.1, max_len=100)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        ## Create causal mask\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x, mask=causal_mask, is_causal=True)  ## Add causal mask!\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_data, epochs=50, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train transformer for next token prediction\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_tensor = torch.LongTensor(train_data)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        acc_list = []\n",
    "        \n",
    "        ## Create batches\n",
    "        indices = torch.randperm(len(train_tensor))\n",
    "        for i in range(0, len(train_tensor), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch = train_tensor[batch_indices]\n",
    "            \n",
    "            ## Input is all tokens except last, target is all tokens except first\n",
    "            input_seq = batch[:, :-1]\n",
    "            target_seq = batch[:, 1:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)  ## shape: (B, L, V)\n",
    "\n",
    "            ## Calculate accuracy for this batch (optional, for monitoring)\n",
    "            with torch.no_grad():\n",
    "                preds = output.argmax(dim=-1)\n",
    "                batch_acc = (preds == target_seq).float().mean().item()\n",
    "                acc_list.append(batch_acc)\n",
    "            \n",
    "            ## Use actual vocabulary size from model output (avoid hard-coded value)\n",
    "            vocab_dim = output.size(-1)\n",
    "            loss = criterion(output.reshape(-1, vocab_dim), target_seq.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / max(1, batch_count)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f} , TrainAcc: {np.mean(acc_list)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformer(model, test_data, batch_size=64, start_eval=None):\n",
    "    \"\"\"Evaluate transformer accuracy (with optional start_eval for position masking)\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    test_tensor = torch.LongTensor(test_data).to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(test_tensor), batch_size):\n",
    "            batch = test_tensor[start:start+batch_size]\n",
    "            if batch.size(0) == 0:\n",
    "                continue\n",
    "            ## Predict all tokens except last, compare to all except first\n",
    "            input_seq = batch[:, :-1]\n",
    "            target_seq = batch[:, 1:]\n",
    "            output = model(input_seq)\n",
    "            preds = output.argmax(dim=-1)\n",
    "            ## If start_eval is set, only evaluate from that position onwards\n",
    "            if start_eval is not None:\n",
    "                preds = preds[:, start_eval:]\n",
    "                target_seq = target_seq[:, start_eval:]\n",
    "            correct += (preds == target_seq).sum().item()\n",
    "            total += target_seq.numel()\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Compare Models with Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training 1-layer transformer\n",
      "==================================================\n",
      "Vocab size: 27 - should be 27\n",
      "Epoch 10, Loss: 2.0020 , TrainAcc: 39.18 %\n",
      "Epoch 20, Loss: 1.8737 , TrainAcc: 42.89 %\n",
      "Epoch 30, Loss: 1.8062 , TrainAcc: 44.56 %\n",
      "Epoch 40, Loss: 1.7606 , TrainAcc: 45.81 %\n",
      "Epoch 50, Loss: 1.7232 , TrainAcc: 46.91 %\n",
      "\n",
      "1-layer transformer accuracy: 0.4619\n",
      "\n",
      "==================================================\n",
      "Training 2-layer transformer\n",
      "==================================================\n",
      "Vocab size: 27 - should be 27\n",
      "Epoch 10, Loss: 1.9082 , TrainAcc: 42.06 %\n",
      "Epoch 20, Loss: 1.7523 , TrainAcc: 46.16 %\n",
      "Epoch 30, Loss: 1.6541 , TrainAcc: 48.92 %\n",
      "Epoch 40, Loss: 1.5901 , TrainAcc: 50.75 %\n",
      "Epoch 50, Loss: 1.5352 , TrainAcc: 52.12 %\n",
      "\n",
      "2-layer transformer accuracy: 0.4836\n",
      "\n",
      "==================================================\n",
      "Training 3-layer transformer\n",
      "==================================================\n",
      "Vocab size: 27 - should be 27\n",
      "Epoch 10, Loss: 1.8779 , TrainAcc: 42.89 %\n",
      "Epoch 20, Loss: 1.7079 , TrainAcc: 47.59 %\n",
      "Epoch 30, Loss: 1.5884 , TrainAcc: 50.83 %\n",
      "Epoch 40, Loss: 1.5026 , TrainAcc: 53.04 %\n",
      "Epoch 50, Loss: 1.4363 , TrainAcc: 54.91 %\n",
      "\n",
      "3-layer transformer accuracy: 0.4918\n"
     ]
    }
   ],
   "source": [
    "## Train models with 1, 2, and 3 layers (use dynamic vocab size derived from train_data)\n",
    "results = {}\n",
    "\n",
    "## Model hyperparameters\n",
    "d_model = 128\n",
    "nhead = 16\n",
    "\n",
    "for num_layers in [1, 2, 3]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {num_layers}-layer transformer\")\n",
    "    print('='*50)\n",
    "\n",
    "    ## derive vocab size from training data to keep consistency\n",
    "    vocab_size = int(max(max(seq) for seq in train_data) + 1)\n",
    "    print(f\"Vocab size: {vocab_size} - should be 27\")\n",
    "    model = SimpleTransformer(vocab_size=vocab_size, num_layers=num_layers, d_model=d_model, nhead=nhead)\n",
    "    train_transformer(model, train_data, epochs=50, lr=0.003)\n",
    "        \n",
    "    accuracy = evaluate_transformer(model, test_data)\n",
    "    results[f\"{num_layers}-layer\"] = accuracy\n",
    "    print(f\"\\n{num_layers}-layer transformer accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ACCURACY COMPARISON\n",
      "==================================================\n",
      "Bigram baseline:     0.4198\n",
      "Trigram baseline:    0.5066\n",
      "1-layer transformer: 0.4619\n",
      "2-layer transformer: 0.4836\n",
      "3-layer transformer: 0.4918\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Compare all models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Bigram baseline:     {bigram_acc:.4f}\")\n",
    "print(f\"Trigram baseline:    {trigram_acc:.4f}\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name} transformer: {acc:.4f}\")\n",
    "\n",
    "## Visualization\n",
    "labels = ['Bigram', 'Trigram'] + list(results.keys())\n",
    "accuracies = [bigram_acc, trigram_acc] + list(results.values())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Next Token Prediction: N-gram vs Transformer')\n",
    "plt.ylim([0, 0.6])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Text Generation Example\n",
    "\n",
    "\n",
    "\n",
    " Let's generate some sequences to see what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9913 , TrainAcc: 39.70 %\n",
      "Epoch 20, Loss: 1.7754 , TrainAcc: 45.76 %\n",
      "Epoch 30, Loss: 1.6459 , TrainAcc: 49.31 %\n",
      "Epoch 40, Loss: 1.5519 , TrainAcc: 51.92 %\n",
      "Epoch 50, Loss: 1.4765 , TrainAcc: 53.84 %\n",
      "\n",
      "Generated sequences (3-layer transformer):\n",
      "Start: 'i am ' -> 'was really for this film that is a a film is '\n",
      "Full:  'i am was really for this film that is a a film is '\n",
      "\n",
      "Start: 'my name is ' -> 'a the first of the beck what is a a bou'\n",
      "Full:  'my name is a the first of the beck what is a a bou'\n",
      "\n",
      "Start: 'i would never ' -> 'to seee this movie i was so the mad '\n",
      "Full:  'i would never to seee this movie i was so the mad '\n",
      "\n",
      "Start: 'this ted talk is about ' -> 'the movie i was a can and a'\n",
      "Full:  'this ted talk is about the movie i was a can and a'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, start_tokens, max_length=15):\n",
    "    \"\"\"Generate sequence from model\"\"\"\n",
    "    model.eval()\n",
    "    generated = start_tokens.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_tokens)):\n",
    "            input_seq = torch.LongTensor([generated])\n",
    "            output = model(input_seq)\n",
    "            next_token = output[0, -1].argmax().item()\n",
    "            generated.append(next_token)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "## Generate from the 3-layer model (use dynamic vocab size)\n",
    "vocab_size = int(max(max(seq) for seq in train_data) + 1)\n",
    "model_3layer = SimpleTransformer(vocab_size=vocab_size, num_layers=3, d_model=d_model, nhead=nhead)\n",
    "train_transformer(model_3layer, train_data, epochs=50)\n",
    "\n",
    "## Let's see what we have learned! \n",
    "start_strings = [\"i am \", \"my name is \", \"i would never \", 'this ted talk is about ']\n",
    "\n",
    "print(\"\\nGenerated sequences (3-layer transformer):\")\n",
    "for start_str in start_strings:\n",
    "    start = [char_to_num(c) for c in start_str]\n",
    "    generated = generate_sequence(model_3layer, start, max_length=50)\n",
    "    gen_str = ''.join(chr(n + ord('a')) if n < 26 else ' ' for n in generated)\n",
    "    print(f\"Start: '{start_str}' -> '{gen_str[len(start_str):]}'\")\n",
    "    print(f\"Full:  '{start_str}{gen_str[len(start_str):]}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-trained Models: BERT and GPT\n",
    "\n",
    "\n",
    "\n",
    " Now let's work with real pre-trained transformer models using the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 BERT: Masked Language Modeling\n",
    "\n",
    "\n",
    "\n",
    " BERT (Bidirectional Encoder Representations from Transformers) is an encoder-only model trained on masked language modeling. It can see the entire context (both left and right) when predicting masked tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_token(text, model, tokenizer, top_k=5):\n",
    "    \"\"\"Predict masked tokens in text\"\"\"\n",
    "    ## Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    ## Find mask position\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    ## Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    ## Get top-k predictions for masked position\n",
    "    mask_predictions = predictions[0, mask_token_index, :]\n",
    "    top_tokens = torch.topk(mask_predictions, top_k, dim=1)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"\\nTop {top_k} predictions:\")\n",
    "    for i, (token_id, score) in enumerate(zip(top_tokens.indices[0], top_tokens.values[0])):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        prob = F.softmax(mask_predictions, dim=1)[0, token_id].item()\n",
    "        print(f\"{i+1}. {token:15s} (probability: {prob:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BERT: Masked Language Modeling\n",
      "============================================================\n",
      "Text: The capital of France is [MASK].\n",
      "\n",
      "Top 5 predictions:\n",
      "1. paris           (probability: 0.4168)\n",
      "2. lille           (probability: 0.0714)\n",
      "3. lyon            (probability: 0.0634)\n",
      "4. marseille       (probability: 0.0444)\n",
      "5. tours           (probability: 0.0303)\n",
      "\n",
      "Text: The cat sat on the [MASK].\n",
      "\n",
      "Top 5 predictions:\n",
      "1. floor           (probability: 0.3145)\n",
      "2. bed             (probability: 0.1190)\n",
      "3. couch           (probability: 0.1070)\n",
      "4. sofa            (probability: 0.0603)\n",
      "5. ground          (probability: 0.0551)\n",
      "\n",
      "Text: Natural [MASK] Processing is a subfield of AI.\n",
      "\n",
      "Top 5 predictions:\n",
      "1. language        (probability: 0.7656)\n",
      "2. information     (probability: 0.0450)\n",
      "3. signal          (probability: 0.0251)\n",
      "4. resource        (probability: 0.0211)\n",
      "5. data            (probability: 0.0103)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load BERT model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "## Test BERT on various examples\n",
    "print(\"=\"*60)\n",
    "print(\"BERT: Masked Language Modeling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predict_masked_token(\"The capital of France is [MASK].\", bert_model, bert_tokenizer)\n",
    "predict_masked_token(\"The cat sat on the [MASK].\", bert_model, bert_tokenizer)\n",
    "predict_masked_token(\"Natural [MASK] Processing is a subfield of AI.\", bert_model, bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BERT for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")  ## Subset for faster training\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "len_data = 10000\n",
    "data_subset = dataset.select(range(len_data))\n",
    "\n",
    "## split data into train and test using hugginface datasets functionalities\n",
    "split = data_subset.train_test_split(test_size=0.7, seed=42)\n",
    "train_txt = split['train']['text']\n",
    "test_txt = split['test']['text']\n",
    "\n",
    "train_labels = split['train']['label']\n",
    "test_labels = split['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_labels=2, freeze_bert=True, lr=2e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "        \n",
    "        ## Freeze BERT encoder\n",
    "        if freeze_bert:\n",
    "            for param in self.model.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.classifier.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_classifier(train_loader, test_loader, epochs=2, lr=2e-4,\n",
    "                          num_labels=2, use_gpu=True, freeze_bert=True):\n",
    "    \"\"\"\n",
    "    Train a BERT text classifier with frozen encoder.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        test_loader: DataLoader for test data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        num_labels: Number of output classes\n",
    "        use_gpu: Whether to use GPU if available\n",
    "    \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ## Initialize model\n",
    "    model = BERTClassifier(num_labels=num_labels, freeze_bert=freeze_bert)\n",
    "    \n",
    "    ## Trainer with optimizations\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        accelerator='gpu' if use_gpu and torch.cuda.is_available() else 'cpu',\n",
    "        precision='16-mixed' if use_gpu and torch.cuda.is_available() else 32,  ## Mixed precision for speed\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=10,\n",
    "        enable_checkpointing=False,\n",
    "        logger=False  ## Disable default logger for simplicity\n",
    "    )\n",
    "    \n",
    "    ## Train\n",
    "    trainer.fit(model, train_loader, test_loader)\n",
    "    \n",
    "    ## Test\n",
    "    test_results = trainer.test(model, test_loader)\n",
    "    ## print(f\"\\nFinal Test Accuracy: {test_results[0]['val_acc']:.3f}\")\n",
    "    print(f\"\\nFinal Test Accuracy: {test_results[0]['test_acc']:.3f}\")\n",
    "    return model\n",
    "\n",
    "def prepare_data(train_txt, train_labels, test_txt, test_labels, max_length=64, batch_size=16):\n",
    "\n",
    "        ## Tokenize\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_enc = tokenizer(train_txt, truncation=True, padding='max_length', \n",
    "                         max_length=max_length, return_tensors='pt')\n",
    "    test_enc = tokenizer(test_txt, truncation=True, padding='max_length',\n",
    "                        max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    ## Create datasets\n",
    "    train_dataset = TensorDataset(train_enc['input_ids'], train_enc['attention_mask'], \n",
    "                                  torch.tensor(train_labels))\n",
    "    test_dataset = TensorDataset(test_enc['input_ids'], test_enc['attention_mask'],\n",
    "                                torch.tensor(test_labels))\n",
    "    \n",
    "    ## DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4,\n",
    "                            pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = prepare_data(\n",
    "    train_txt, train_labels, test_txt, test_labels, max_length=64, batch_size=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX 2000 Ada Generation Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                          | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model   | BertForSequenceClassification | 109 M  | eval \n",
      "1 | loss_fn | CrossEntropyLoss              | 0      | train\n",
      "------------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "231       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966c0502658d43a09b44a63664b35e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4551b2afae24a18add381897a3ab229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96b023893154e8f9426c0cf579bc06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1c015d83c247a1b69028b942d39ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf0a3630a0a40fd83c3da007176925a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08cb8e29d0449cf95684a943851e06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb4fb7cb4a94dfc9080b8cf9da0039b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b361673d14196a3f02e96b87132ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">        Test metric        </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6324285864830017     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6442899107933044     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6324285864830017    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6442899107933044    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 0.632\n"
     ]
    }
   ],
   "source": [
    "model = train_bert_classifier(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=5,\n",
    "    lr=1e-5,\n",
    "    freeze_bert=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GPT: Text Generation\n",
    " GPT (Generative Pre-trained Transformer) is a decoder-only model trained on causal language modeling. It can only see the left context when predicting the next token, making it suitable for text generation.\n",
    " One of the most popular AI models today, ChatGPT, is a type of Transformer Decoder.\n",
    " As you might have noticed when we generate text via a Decoder this is relatively slow. The reason for this is that for every \"forward pass\", the model only predicts the next token, which we then feed back to the input and make it predict the following token. In the example image below we start with an input we provide \"Hello, my name\", which are all fed to the input to predict the next token, where the model says \" is\". Now for the next word we need to feed the string \"Hello, my name is\" to the model, so it predicts \" James\". As you can see this takes a lot of iterations through the model. Through clever computing we can store some of the matrix multiplications that are happening inside the model so when we run `model.generate()` we don't have to redo the computations we could already store when we predicted previous tokens. \n",
    "\n",
    "\n",
    "\n",
    " <p align=\"center\">\n",
    "    <img src=\"https://www.researchgate.net/publication/363765406/figure/fig1/AS:11431281085833161@1663902465850/llustration-of-transformer-based-text-generation.png\" alt=\"gpt_generation\" style=\"max-width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_gpt(prompt, model, tokenizer, max_length=50, temperature=0.7):\n",
    "    \"\"\"Generate text from prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPT-2: Text Generation\n",
      "============================================================\n",
      "\n",
      "Prompt: Natural language processing is\n",
      "Generated: Natural language processing is a powerful and valuable tool in the development of languages, but it is still limited to languages that are easy to understand and understand.\n",
      "\n",
      "There are currently no native languages that can\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: The transformer architecture revolutionized\n",
      "Generated: The transformer architecture revolutionized the way we make the electrical system. The power grid can be made of a single or multiple pieces of wire, but they all have a common core. The transformer's core\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: In the field of linguistics,\n",
      "Generated: In the field of linguistics, we often get the impression that language is more like a language than an object. As a result, we often get the impression that language is more like a language than\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Load GPT-2 model and tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt_model.eval()\n",
    "\n",
    "## Set pad token (GPT2 doesn't have one by default)\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "\n",
    "## Test GPT on various prompts\n",
    "print(\"=\"*60)\n",
    "print(\"GPT-2: Text Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts = [\n",
    "    \"Natural language processing is\",\n",
    "    \"The transformer architecture revolutionized\",\n",
    "    \"In the field of linguistics,\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generate_text_gpt(prompt, gpt_model, gpt_tokenizer, max_length=40)}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Key Differences: BERT vs GPT\n",
    "\n",
    " | Aspect | BERT | GPT |\n",
    " |--------|------|-----|\n",
    " | Architecture | Encoder-only | Decoder-only |\n",
    " | Attention | Bidirectional | Causal (left-to-right) |\n",
    " | Training Task | Masked Language Modeling | Next Token Prediction |\n",
    " | Best For | Understanding tasks (classification, NER) | Generation tasks |\n",
    " | Context | Sees full sentence | Sees only previous tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    " In this notebook, we explored:\n",
    " 1. **Attention Mechanism**: The core innovation that allows transformers to weigh the importance of different parts of the input\n",
    " 2. **Multi-Head Attention**: Running multiple attention operations in parallel to capture different types of relationships\n",
    " 3. **Toy Experiments**: How transformers of different depths compare to n-gram baselines on next token prediction\n",
    " 4. **Pre-trained Models**:\n",
    "    - BERT for bidirectional context understanding (masked language modeling)\n",
    "    - GPT for text generation (causal language modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    " - Vaswani et al. (2017). \"Attention is All You Need\". NeurIPS.\n",
    " - Devlin et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". NAACL.\n",
    " - Radford et al. (2019). \"Language Models are Unsupervised Multitask Learners\" (GPT-2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
