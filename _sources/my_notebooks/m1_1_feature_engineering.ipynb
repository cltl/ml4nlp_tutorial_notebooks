{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.1 Feature Engineering \n",
    "[![View notebooks on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/cltl/ml4nlp_tutorial_notebooks/blob/main/my_notebooks/m1_1_feature_engineering.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cltl/ml4nlp_tutorial_notebooks/blob/main/my_notebooks/m1_1_feature_engineering.ipynb)  \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "By working through this notebook, you will learn:\n",
    "1. What features are and the different types that exist\n",
    "2. How to use DictVectorize to process and transform features\n",
    "\n",
    "## Introduction to Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy.\n",
    "\n",
    "**Why is it important?**\n",
    "- Raw data is often not in a format suitable for machine learning algorithms \n",
    "- Good features can make simple models perform extremely well\n",
    "- By extracting features we can understand our data a lot better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Features\n",
    "\n",
    "### What is a Feature?\n",
    "\n",
    "A feature is an individual measurable property or characteristic of a phenomenon being observed. In machine learning, features are the input variables used to make predictions. For problems in NLP what a feature to use is heavily dependend on the task we are trying to complete. For example for named entity recognition, where we aim to check if a word or substring refers to a named entity, it might be relevant if this string starts with a capital letter. Of course since many other non-entity words also start with a capital (such as the first word in a sentence), based on the feature `is_capitalized` we might now know for sure if the word refers to a named entity, thus it is useful to have many different features. In general we distiguish between a few different types of features based on the values it can take, below is a list of the different categories of features.\n",
    "\n",
    "### Types of Features\n",
    "1. **Numerical Features**: Continuous or discrete numbers\n",
    "2. **Categorical Features**: Discrete categories or labels\n",
    "3. **Binary Features**: True/False, Yes/No, 1/0\n",
    "4. **Text Features**: Words, characters, n-grams\n",
    "\n",
    "**Question**: Can you identify the type of features for the instance below?\n",
    "```python\n",
    "house_instance = {\n",
    "    'city': 'Amsterdam', \n",
    "    'rooms': 3,\n",
    "    'area_m2': 75.0, \n",
    "    'type': 'apartment'\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Numerical Features\n",
    "Let's start with example of numerical data, taking the NLP side a bit more loosly and focusing on a more general machine learning problem to keep it simpler. Let's say we have some measurements from different weather stations in the Netherlands, for each weather station we have some information stored: temperature and humidity measurements. See the python cell below. \n",
    "<details>\n",
    "  <summary>Refresher: instance, keys, values\n",
    "  </summary>\n",
    "\n",
    "So each **instance** in your dataset is a dictionary. A dictionary contains keys and values. For this toy example an instance is a weather station, the **keys** are the features (e.g. temperature, humidity, city), and the **values** (as the name indicates) are what value that feature has for a specific instance, for a specific weather station the value for the key \"temperature\" might be 18.5 (probably in degrees).\n",
    "</details>\n",
    " \n",
    "\n",
    "**Note:** For NLP feature engineering and in these exercises we will _first_ store the features in a python dictionary (as it's easier to control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature and humidity measurements\n",
    "measurements = [\n",
    "    {'temperature': 18.5, 'humidity': 75},\n",
    "    {'temperature': 22.0, 'humidity': 60},\n",
    "    {'temperature': 15.2, 'humidity': 80},\n",
    "    {'temperature': 20.1, 'humidity': 65},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data: `DictVectorizer` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers however love numbers, so we want to simplify this it and convert each dictionary to a vector, which is again just a list of numbers. Therefore for each weather station we obtain a vector.\n",
    "For our experiments a nice existing package we can use is from `sklearn`, a python class `DictVectorizer`, which makes it really easy to convert our dictionaries to vectors.z\n",
    "\n",
    "Let's go and vectorize our `measurements`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warning messages for cleaner output of the website\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix is of type: <class 'numpy.ndarray'>\n",
      "Feature matrix: \n",
      " [[75.  18.5]\n",
      " [60.  22. ]\n",
      " [80.  15.2]\n",
      " [65.  20.1]]\n",
      "\n",
      "Feature names: ['humidity' 'temperature']\n"
     ]
    }
   ],
   "source": [
    "# Remember to set sparse=False, otherwise you get a weird type of variable that is annoying to work with (we discuss this later)\n",
    "vec_num = DictVectorizer(sparse=False)  \n",
    "X_num = vec_num.fit_transform(measurements)\n",
    "\n",
    "print(\"Feature matrix is of type:\", type(X_num))\n",
    "print(\"Feature matrix: \\n\", X_num)\n",
    "print(\"\\nFeature names:\", vec_num.get_feature_names_out())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?** When values are numerical (int or float), DictVectorizer treats them as **numerical features** and keeps them as-is. Each feature becomes one column in the matrix. Simple!\n",
    "\n",
    "<details>\n",
    "  <summary><b>Question</b> : What are the columns? </summary>\n",
    "\n",
    "The columns represent the dictionary features, but remember that dictionaries don't really have an order (which also makes them efficient via cool math tricks), thus to understand what each column represents we use the function `vec.get_feature_names_out()`, which shows us what each feature each row is.  \n",
    "-  `humidity` → 1 column\n",
    "- `temperature` → 1 column\n",
    "- **Total: 2 columns**\n",
    "\n",
    "Tbh not as relevant here, but more relevant for categorical values\n",
    "</details>\n",
    "\n",
    "\n",
    "Also the option `sparse=True` ensures that the returned object is a numpy array, which are very easy to work with. If you don't set this option you get the `scipy.sparse._csr.csr_matrix` object, which is more \"memory efficient\" but much harder to work with/modify.\n",
    "\n",
    "<details>\n",
    "  <summary><b>Extra</b> : How does DictVectorizer remember the features? </summary>\n",
    "\n",
    "In python usually if we have a variable like `vec_num` this value does not change unless it is overwritten. \n",
    "But it seems that in the code above `vec_num` is created in line 2, then it is used in line 3 via `fit_transform()`, but then in line 7  using `get_feature_names_out()` it seems to have remember the features. \n",
    "\n",
    "The important part is that `DictVectorizer` is a class object, and `vec_num` is an instantiation of that class, which can store variables as well. So while it seems that the line `fit_transform()` only returns `X_num` it also  _stores_ the feature names. \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Categorical Values\n",
    "Now let's look at categorical (string) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data with categorical features\n",
    "weather_data = [\n",
    "    {'city': 'Amsterdam', 'weather': 'sunny', 'wind': 'strong'},\n",
    "    {'city': 'Rotterdam', 'weather': 'cloudy', 'wind': 'weak'},\n",
    "    {'city': 'Utrecht', 'weather': 'rainy', 'wind': 'weak'},\n",
    "    {'city': 'Amsterdam', 'weather': 'snowy', 'wind': 'weak'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vectorizing Categorical Data -> _One-Hot Encoding_\n",
    "For many types of features we don't have the exact numbers, but instead we have different types of values it can take. For example the feature \"city\" can be \"Amsterdam\", \"Rotterdam\" and \"Utrecht\", so it can take 3 different categories. Again for our Machine Learning experiments we want to convert this to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix is of shape: (4, 9)\n",
      "Feature matrix: \n",
      " [[1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the vectorizer\n",
    "vec_cat = DictVectorizer(sparse=False)  # sparse=False to get dense array for visualization\n",
    "X_cat = vec_cat.fit_transform(weather_data)\n",
    "\n",
    "print(\"Feature matrix is of shape:\", X_cat.shape)   # Shape returns (number of rows, number of columns). \n",
    "print(\"Feature matrix: \\n\", X_cat)\n",
    "# print(\"\\nFeature names:\", vec_cat.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- ### Understanding the Output: One-Hot Encoding -->\n",
    "\n",
    "**What happened?** DictVectorizer detected that all values are strings and treated them as **categorical features**. \n",
    "For categorical features it creates a separate collumn for each value. This representation with is called **one-hot encoding** as it creates a binary (0 or 1) feature for each unique value, but the model is 1 at the place of that value and 0 of the other places.\n",
    "\n",
    "<!-- #### How Many Columns Are Created? -->\n",
    "\n",
    "So for categorical features, the number of columns equals the number of unique values:\n",
    "- `city` has 3 unique values (Amsterdam, Rotterdam, Utrecht) → **3 columns**\n",
    "- `weather` has 3 unique values (sunny, cloudy, rainy, snowy) → **4 columns**\n",
    "- `wind` has 3 unique values (strong, weak) → **2 columns**\n",
    "- **Total: 9 columns**\n",
    "\n",
    "<!-- #### Reading the Feature Matrix -->\n",
    "\n",
    "Let's look at the first row (Amsterdam, sunny, strong):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First instance features:\n",
      "{'city': 'Amsterdam', 'weather': 'sunny', 'wind': 'strong'}\n",
      "\n",
      "First row of feature matrix:\n",
      "[1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "\n",
      "Feature names:\n",
      "['city=Amsterdam' 'city=Rotterdam' 'city=Utrecht' 'weather=cloudy'\n",
      " 'weather=rainy' 'weather=snowy' 'weather=sunny' 'wind=strong' 'wind=weak']\n"
     ]
    }
   ],
   "source": [
    "print(\"First instance features:\")\n",
    "print(weather_data[0])\n",
    "print(\"\\nFirst row of feature matrix:\")\n",
    "print(X_cat[0])\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec_cat.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `[1, 0, 0, 1, 0, 0, 0, 1, 0]` means:\n",
    "- `city=Amsterdam`: 1 (others are 0)\n",
    "- `weather=rainy`: 1 (others are 0)\n",
    "- `wind=strong`: 1 (others are 0)\n",
    "\n",
    "Each instance has exactly one '1' per original feature, indicating which category it belongs to.\n",
    "\n",
    "Second check. For the 1st column we see that besided the first row, also the last row has the value 1. We can see that this is true because the last item in our `weather_data` list also has 'Amsterdam' as the city.\n",
    "\n",
    "As you may have realized by now, reading one-hot vectors is a tad anoying, it is good practice that you understand what is going on, but you may now understand why we prefer to read and create our feature data in dictionaries first. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: When Categorical Values Go Wrong\n",
    "\n",
    "What happens if we accidentally have categorical data stored as numbers? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix (WRONG!):\n",
      "[[1. 0. 1. 0.]\n",
      " [2. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [3. 0. 1. 0.]]\n",
      "\n",
      "Feature names:\n",
      "['city_code' 'weather=cloudy' 'weather=rainy' 'weather=sunny']\n"
     ]
    }
   ],
   "source": [
    "# Oops! City codes stored as numbers\n",
    "bad_data = [\n",
    "    {'city_code': 1, 'weather': 'rainy'},   # 1 = Amsterdam\n",
    "    {'city_code': 2, 'weather': 'sunny'},   # 2 = Rotterdam\n",
    "    {'city_code': 1, 'weather': 'cloudy'},  # 1 = Amsterdam\n",
    "    {'city_code': 3, 'weather': 'rainy'},   # 3 = Utrecht\n",
    "]\n",
    "\n",
    "vec_bad = DictVectorizer(sparse=False)\n",
    "X_bad = vec_bad.fit_transform(bad_data)\n",
    "\n",
    "print(\"Feature matrix (WRONG!):\")\n",
    "print(X_bad)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec_bad.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Problem!** \n",
    " - DictVectorizer treats `city_code` as a numerical feature because the values are numbers. But city codes are categorical, so while we can still distinghuish the cities in this way, it creates a ordering right now that is likely very random to us, but for the machine means a lot. \n",
    " - Due to this arbirary ordering it seems both that city 2 (Rotterdam) is is \"twice as much\" as city 1 (Amsterdam), something that will not sit right with people form Amsterdam. Moreover, the classification algorithms we will train later on can not really separate the classes right now, as the effect of the other city codes mess with eachother. So let's fix this again.\n",
    "\n",
    "**The fix:** Convert categorical data to strings before vectorizing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix (CORRECT!):\n",
      "[[1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0.]]\n",
      "\n",
      "Feature names:\n",
      "['city_code=1' 'city_code=2' 'city_code=3' 'weather=cloudy'\n",
      " 'weather=rainy' 'weather=sunny']\n"
     ]
    }
   ],
   "source": [
    "# Fix: Convert to strings\n",
    "good_data = [\n",
    "    {'city_code': '1', 'weather': 'rainy'},\n",
    "    {'city_code': '2', 'weather': 'sunny'},\n",
    "    {'city_code': '1', 'weather': 'cloudy'},\n",
    "    {'city_code': '3', 'weather': 'rainy'},\n",
    "]\n",
    "\n",
    "vec_good = DictVectorizer(sparse=False)\n",
    "X_good = vec_good.fit_transform(good_data)\n",
    "\n",
    "print(\"Feature matrix (CORRECT!):\")\n",
    "print(X_good)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec_good.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now `city_code` is properly one-hot encoded with 3 columns!\n",
    "\n",
    "**Key lesson:** Always ensure your categorical variables are stored as strings, not numbers, before using DictVectorizer. But also don't forget that for some features you really do want the numerical values, for example for an NLP task where we want to store a feature `word_length` to count then number of characters in a word, we really do want to use an integer number as scale means something here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Mixed Features\n",
    "\n",
    "Real-world data often has both categorical and numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix:\n",
      "[[ 75.   1.   0.   0.   3.   1.   0.]\n",
      " [120.   0.   1.   0.   4.   0.   1.]\n",
      " [ 55.   0.   0.   1.   2.   1.   0.]\n",
      " [150.   1.   0.   0.   5.   0.   1.]]\n",
      "\n",
      "Feature names:\n",
      "['area_m2' 'city=Amsterdam' 'city=Rotterdam' 'city=Utrecht' 'rooms'\n",
      " 'type=apartment' 'type=house']\n"
     ]
    }
   ],
   "source": [
    "# Housing data (simplified)\n",
    "housing_data = [\n",
    "    {'city': 'Amsterdam', 'rooms': 3, 'area_m2': 75.0, 'type': 'apartment'},\n",
    "    {'city': 'Rotterdam', 'rooms': 4, 'area_m2': 120.0, 'type': 'house'},\n",
    "    {'city': 'Utrecht', 'rooms': 2, 'area_m2': 55.0, 'type': 'apartment'},\n",
    "    {'city': 'Amsterdam', 'rooms': 5, 'area_m2': 150.0, 'type': 'house'},\n",
    "]\n",
    "\n",
    "vec_mixed = DictVectorizer(sparse=False)\n",
    "X_mixed = vec_mixed.fit_transform(housing_data)\n",
    "\n",
    "print(\"Feature matrix:\")\n",
    "print(X_mixed)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec_mixed.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Column count breakdown:**\n",
    "- `area_m2` (numerical) → 1 column\n",
    "- `city` (3 unique values) → 3 columns\n",
    "- `rooms` (numerical) → 1 column\n",
    "- `type` (2 unique values) → 2 columns\n",
    "- **Total: 7 columns**\n",
    "\n",
    "Notice how `city` and `type` (strings) are one-hot encoded, while `rooms` and `area_m2` (numbers) remain as single numerical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sparse vs Dense Representation\n",
    "\n",
    "When you have many categorical features with many possible values, most entries in the feature matrix are zeros. This creates **sparse** matrices (think of it that most of the values are 0). Let's see the difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix shape: (50, 31) - Type of the variable: <class 'numpy.ndarray'>  (classic Numpy array, which we like)\n",
      "Dense matrix size in memory: 12400 bytes\n",
      "\n",
      "Sparse matrix shape: (50, 31) - Type of the variable: <class 'scipy.sparse._csr.csr_matrix'>  (weird Scipy sparse matrix type)\n",
      "Sparse matrix size in memory: 2004 bytes\n",
      "\n",
      "Memory savings: 83.8%\n"
     ]
    }
   ],
   "source": [
    "# Create a larger dataset with more categories\n",
    "large_data = [\n",
    "    {'city': f'City_{i%10}', 'district': f'District_{i%20}', 'price': i*1000}\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "# Dense representation\n",
    "vec_dense = DictVectorizer(sparse=False)\n",
    "X_dense = vec_dense.fit_transform(large_data)\n",
    "\n",
    "# Sparse representation (default)\n",
    "vec_sparse = DictVectorizer(sparse=True)\n",
    "X_sparse = vec_sparse.fit_transform(large_data)\n",
    "\n",
    "print(f\"Dense matrix shape: {X_dense.shape} - Type of the variable: {type(X_dense)}  (classic Numpy array, which we like)\")\n",
    "print(f\"Dense matrix size in memory: {X_dense.nbytes} bytes\")\n",
    "print(f\"\\nSparse matrix shape: {X_sparse.shape} - Type of the variable: {type(X_sparse)}  (weird Scipy sparse matrix type)\")\n",
    "print(f\"Sparse matrix size in memory: {X_sparse.data.nbytes + X_sparse.indptr.nbytes + X_sparse.indices.nbytes} bytes\")\n",
    "print(f\"\\nMemory savings: {(1 - (X_sparse.data.nbytes + X_sparse.indptr.nbytes + X_sparse.indices.nbytes) / X_dense.nbytes) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse matrices** only store non-zero values, making them much more memory-efficient. This is especially important when working with large datasets or many categorical features. However, when we create Machine Learning algorithms we often do want the original sparse vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Unknown Values\n",
    "Untill now our DictVectorizer used the function `vec.fit_transform()`, which is nice in the sense that it is only 1 line of code (and we coders are lazy by nature), but for our experiments this is a problem. \n",
    "When we have different dataset splits, lets say our training data, test data, or validation data. Then perhaps we don't want to do fit() and transform() on each split separately, but use it on each of them combined. \n",
    "An issue that could appear if we do `vec.fit_transform()` on each split separately is 1) What if some values are not in one split but do appear in the others? 2) What if the ordering of our features is different for each?\n",
    "Let's see some of these issues in practice to understand them better.\n",
    "<!-- But it is crucial here that the columns of them match, and that it is complete, that is we have enough column to account for all our features (numerical or catagorical). -->\n",
    "\n",
    "What happens when new data has categories not seen during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error - new feature values in test are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features (Shape is: (2, 4)):\n",
      "[[1. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "\n",
      "Feature names:\n",
      "['city=Amsterdam' 'city=Rotterdam' 'weather=rainy' 'weather=sunny']\n"
     ]
    }
   ],
   "source": [
    "# Original training data\n",
    "train_data = [\n",
    "    {'city': 'Amsterdam', 'weather': 'rainy'},\n",
    "    {'city': 'Rotterdam', 'weather': 'sunny'},\n",
    "]\n",
    "\n",
    "# New test data with unseen category\n",
    "test_data = [\n",
    "    {'city': 'Amsterdam', 'weather': 'sunny'},\n",
    "    {'city': 'Den Haag', 'weather': 'rainy'},  # New city!\n",
    "]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(train_data)\n",
    "\n",
    "# we could also do fit and transform in one go with:\n",
    "# X_train = vec.fit_transform(train_data)\n",
    "\n",
    "# Transform test data - unseen values are ignored by default\n",
    "X_test = vec.transform(test_data)\n",
    "\n",
    "print(f\"Test features (Shape is: {X_test.shape}):\")\n",
    "print(X_test)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 'Den Haag' is silently ignored. All its columns for the city feature are zeros, because 'Den Haag' wasn't in the training data, so when we fitted our DictVectorizer it was not aware of Den Haag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Ensure that the DictVectorizer is fit on all the data (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features (Shape is: (2, 5)):\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]]\n",
      "\n",
      "Feature names:\n",
      "['city=Amsterdam' 'city=Den Haag' 'city=Rotterdam' 'weather=rainy'\n",
      " 'weather=sunny']\n"
     ]
    }
   ],
   "source": [
    "# Original training data\n",
    "train_data = [\n",
    "    {'city': 'Amsterdam', 'weather': 'rainy'},\n",
    "    {'city': 'Rotterdam', 'weather': 'sunny'},\n",
    "]\n",
    "\n",
    "# New test data with unseen category\n",
    "test_data = [\n",
    "    {'city': 'Amsterdam', 'weather': 'sunny'},\n",
    "    {'city': 'Den Haag', 'weather': 'rainy'},  # New city!\n",
    "]\n",
    "\n",
    "# We combine the lists of dictionaries\n",
    "full_data = train_data + test_data\n",
    "\n",
    "# We just do fit, not transforming yet, as fitting can be bit faster than fit_transform\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(full_data)\n",
    "\n",
    "# Transform test data - this time 'Den Haag' is known to the vectorizer \n",
    "X_test = vec.transform(test_data)\n",
    "\n",
    "print(f\"Test features (Shape is: {X_test.shape}):\")\n",
    "print(X_test)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our `vec.transform()` returns vectors trained on the full dataset, so that each vector has 5 rows, also one for \"Den Haag\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Self-check questions:\n",
    "- What are the different types of features in machine learning?\n",
    "- How are the different features transformed when we vectorize them?\n",
    "- How does `DictVectorize` work, what does `vec.fit()`, `vec.transform()`,  and `vec.fit_transform()` do?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
