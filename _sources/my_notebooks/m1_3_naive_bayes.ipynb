{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.3 Naive Bayes Classifier\n",
    " [![View notebooks on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/cltl/ml4nlp_tutorial_notebooks/blob/main/my_notebooks/m1_3_naive_bayes.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cltl/ml4nlp_tutorial_notebooks/blob/main/my_notebooks/m1_3_naive_bayes.ipynb)  \n",
    "\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "By working through this notebook, you will learn:\n",
    "1. Basics of probability theory: prior probabilities, conditional probabilities\n",
    "2. What Bayes' theorem is and how to use it to calculate the likelihood of an event \n",
    "3. What the naive bayes assumption is\n",
    "2. How we can construct a Naive Bayes' Classifier and use it for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warning messages for cleaner output of the website\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Counter is a useful package takes as input a list of elements (e.g. a list of words in a text) and \n",
    "#  returns a dictionary how many times each element occurs with key (e.g. \"apple\" ): value ( e.g. 5) .\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Machine Learning often boiles down to simply learning the statistics of our datasets. For many especially for classification tasks, we want to know: \n",
    "\n",
    "What is the probability that:\n",
    "1. This sentiment of this review is positive/negative?\n",
    "2. That this email is spam?\n",
    "3. This social media post contains hate speech?\n",
    "\n",
    "Luckily we don't need to reinvent the wheel and can borrow much theory from probability theory. \n",
    "In this notebook we will walk through the steps of computing probabilities, discuss a few different type of probabilities and finally use it to constructe an algorithm we can use for classification, the **Naive Bayes Classifier**. Despite its simplicity, Naive Bayes often performs remarkably well for NLP tasks like spam detection, sentiment analysis, and document categorization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding Probabilities & Bayes' Theorem \n",
    "\n",
    "Imagine you receive an email with the words: \"winner\", \"free\", \"click\", \"now\".\n",
    "Your intuition probably tells you this is spam! Why? Because you've learned from experience that these words appear much more frequently in spam than in legitimate emails.\n",
    "Naive Bayes formalizes this intuition using probability theory.\n",
    "\n",
    "**Core Question:** Given a text document, we want to answer: **What is the probability that this document belongs to class C?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Task Formalization\n",
    "\n",
    "Let's say we have a list of documents: \n",
    "- The **Evidence**: `Documents = [document_1, document_2, ..., document_3]`. \n",
    "- The **Classes**: `Classes = [spam, not_spam]`\n",
    "\n",
    "#### Bag-Of-Words (BOW)\n",
    "- For now also let's assume that each document is represented as a **Bag-Of-Words (BOW)** \n",
    "- In the BOW we neglect the order of the words in a document and only count for each word in the document how often it appears. As you may realize we loose a lot of information this way, but it makes it very easy to process text for simple machine learning models. \n",
    "\n",
    "![bow_fig](https://miro.medium.com/v2/0*cf1wq8eIix-Z2qIf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 8, 'and': 6, 'i': 5, 'you': 4, 'be': 4, 'were': 3, 'see': 3, 'we': 2, 'my': 2, 'on': 2, 'a': 2, \"i'll\": 2, 'both': 1, 'young': 1, 'when': 1, 'first': 1, 'saw': 1, 'close': 1, 'eyes': 1, 'flashback': 1, 'starts': 1, \"i'm\": 1, 'standing': 1, 'there': 1, 'balcony': 1, 'in': 1, 'summer': 1, 'air': 1, 'lights,': 1, 'party,': 1, 'ball': 1, 'gowns': 1, 'make': 1, 'your': 1, 'way': 1, 'through': 1, 'crowd': 1, 'say': 1, 'hello': 1, 'little': 1, 'did': 1, 'know': 1, 'that': 1, 'romeo,': 1, 'throwing': 1, 'pebbles': 1, 'daddy': 1, 'said,': 1, \"'stay\": 1, 'away': 1, 'from': 1, \"juliet'\": 1, 'was': 1, 'crying': 1, 'staircase': 1, 'begging': 1, 'you,': 1, \"'please\": 1, \"don't\": 1, \"go,'\": 1, 'said': 1, \"'romeo,\": 1, 'take': 1, 'me': 1, 'somewhere': 1, 'can': 1, 'alone': 1, 'waiting,': 1, 'all': 1, \"that's\": 1, 'left': 1, 'to': 1, 'do': 1, 'is': 1, 'run': 1, \"you'll\": 1, 'prince': 1, 'princess': 1, \"it's\": 1, 'love': 1, 'story,': 1, 'baby,': 1, 'just': 1, 'say,': 1, \"'yes''\": 1})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_bag_of_words(text):\n",
    "    \"\"\" We want to count how many words there are in the text\"\"\"\n",
    "    words = text.strip().lower().split()\n",
    "    return Counter(words)\n",
    "\n",
    "# Example text: biggest pop song of all time\n",
    "example_text = \"We were both young when I first saw you \\n I close my eyes and the flashback starts \\n I'm standing there \\n On a balcony in summer air\\n See the lights, see the party, the ball gowns \\n See you make your way through the crowd \\n And say hello \\n Little did I know \\n That you were Romeo, you were throwing pebbles \\n And my daddy said, 'Stay away from Juliet' \\n And I was crying on the staircase \\n Begging you, 'Please don't go,' and I said \\n 'Romeo, take me somewhere we can be alone \\n I'll be waiting, all that's left to do is run \\n You'll be the prince and I'll be the princess \\n It's a love story, baby, just say, 'Yes''\"\n",
    "print(get_bag_of_words(example_text))\n",
    "\n",
    "def plot_top_counts(counter, N=10):\n",
    "    \"\"\"Plot the counts of the top N elements in a Counter object.\"\"\"\n",
    "    top_items = counter.most_common(N)\n",
    "    labels, counts = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Top {N} Word Counts in Love Story Excerpt')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example (thought the word count distribution is a bit boring here since the text is so short):\n",
    "word_counts = get_bag_of_words(example_text)\n",
    "plot_top_counts(word_counts, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Types of probabilities\n",
    "\n",
    "Imagine we have a tiny dataset of 10 emails: 2 are spam, 8 are legitimate.\n",
    "\n",
    "**Prior Probability P(Class):** The baseline probability of a class before seeing any evidence.\n",
    "- P(spam) = 6/10 = 0.2\n",
    "- P(not spam) = 4/10 = 0.8\n",
    "\n",
    "These tell us: \"Without reading the email, what are the chances it's spam?\"\n",
    "\n",
    "Another type of probabilities are  **Conditional Probabilities**, written as P(A|B), what is the probability of A _given_ I already know B to be true.\n",
    "For NB classification we distinghuish between two types of conditional probabilities, as they tell us something different for classification. While it is good to understand these terms know that probability theory does not really care what values we are talking about, it can just be any abstract A or B. For classifcation though we want to predict the probability of a _Class_, given the observed _Evidence_ . \n",
    "\n",
    "Thus we disthinghuish between the two conditional probabilities:\n",
    "1. **Likelihood P(Evidence | Class):** How likely is our evidence if we know the class?\n",
    "    - So if we see the word \"FREE\" in an email, we ask: \"How often does 'FREE' appear in spam (or in non-spam emails)?\"\n",
    "2. **Posterior Probability P(Class | Evidence):** What we actually want! Given the evidence (the email content), what's the probability of each class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bayes' Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bayes' Theorem lets us flip conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(\\text{Class} \\mid \\text{Evidence}) = \\frac{P(\\text{Evidence} \\mid \\text{Class}) \\times P(\\text{Class})}{P(\\text{Evidence})}\n",
    "$$\n",
    "\n",
    "In plain English: \n",
    "> The probability of spam given this email = (How likely this email is if it's spam × How common spam is) / How common this email is in general\n",
    "\n",
    "Since we're comparing classes, $P(\\text{Evidence})$ is the same for both, so we can ignore it:\n",
    "\n",
    "$$\n",
    "P(\\text{Class} \\mid \\text{Evidence}) \\propto P(\\text{Evidence} \\mid \\text{Class}) \\times P(\\text{Class})\n",
    "$$\n",
    "\n",
    "**The \"Naive\" Assumption:** To make this practical, we assume all words are independent. For an email with words $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$\n",
    "P(\\text{Email} \\mid \\text{Class}) = P(w_1 \\mid \\text{Class}) \\times P(w_2 \\mid \\text{Class}) \\times \\ldots \\times P(w_n \\mid \\text{Class})\n",
    "$$\n",
    "\n",
    "This is clearly false in reality (for example if you see \"free\" you're more likely to see \"prize\"), but it works surprisingly well!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. A Manual Example of applying Bayes' Theorem \n",
    "It is good to work out some of these examples on paper to really understand what is happening. Check [this website](https://www.lesswrong.com/w/bayes-rule?lens=high-speed-intro-to-bayes-s-rule) for some further material and examples. I copied one example below, if you don't get the answer immidiately that totally normal. Give yourself some time to work it out. \n",
    "\n",
    "\n",
    "Suppose you're screening a set of patients for a disease, which we'll call Diseasitis.[1] Your initial test is a tongue depressor containing a chemical strip, which usually turns black if the patient has Diseasitis.\n",
    "\n",
    "- Based on prior epidemiology, you expect that around 20% of patients in the screening population have Diseasitis.\n",
    "- Among patients with Diseasitis, 90% turn the tongue depressor black.\n",
    "- 30% of the patients without Diseasitis will also turn the tongue depressor black.\n",
    "\n",
    "What fraction of patients with black tongue depressors have Diseasitis?\n",
    "\n",
    "\n",
    "\n",
    " <!-- <details open> -->\n",
    " <details>\n",
    "   <summary><b>Answer</b>: Don't look without coming up with your own solution first </summary>\n",
    "\n",
    "3/7 or 43%, quickly obtainable as follows: In the screened population, there's 1 sick patient for 4 healthy patients. Sick patients are 3 times more likely to turn the tongue depressor black than healthy patients. (1:4)⋅(3:1)=(3:4) or 3 sick patients to 4 healthy patients among those that turn the tongue depressor black, corresponding to a probability of 3/7=43% that the patient is sick.\n",
    "\n",
    " </details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3: Naive Bayes' Classifier for Spam detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load & Process Dataset: Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded! Number of examples: 8175\n",
      "\n",
      "Number of examples after selection: 100\n"
     ]
    }
   ],
   "source": [
    "# Load a spam detection dataset from huggingface datasets. \n",
    "# Again URL is obtained from link in load_dataset: https://huggingface.co/datasets/Deysi/spam-detection-dataset\n",
    "full_dataset = load_dataset('Deysi/spam-detection-dataset', split='train')  # Load only first 100 examples for speed\n",
    "print(f\"\\nDataset loaded! Number of examples: {len(full_dataset)}\")\n",
    "\n",
    "# shuffle and take the first 1000 examples for quicker processing\n",
    "full_dataset = full_dataset.shuffle(seed=42)\n",
    "dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\nNumber of examples after selection: {len(dataset)}\")\n",
    "\n",
    "labels = [item['label'] for item in dataset]  # labelse are \"spam\" or \"not_spam\"\n",
    "text_data = [item['text'] for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Example text and label:\n",
      "- Example text:\n",
      " \"\"\"Looking for quick cash? Look no further!\n",
      " Join our exclusive money-making community and start earning thousands today.\n",
      " All you need is a computer and internet connection!\n",
      " Don't miss out on this amazing opportunity.\n",
      "\"\"\"\n",
      "- Example label: spam\n"
     ]
    }
   ],
   "source": [
    "example = dataset[0]\n",
    "\n",
    "# we can also print it as is, but this is a bit easier to read (put all sentences below each other)\n",
    "example_text = example['text']\n",
    "example_text_formatted = example_text.strip().replace('.', '.\\n').replace('!', '!\\n')\n",
    "print(\"## Example text and label:\")\n",
    "print(f\"- Example text:\\n \\\"\\\"\\\"{example_text_formatted}\\\"\\\"\\\"\")\n",
    "print(f\"- Example label: {example['label']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing Prior Probabilities `P(Class)` \n",
    "<!-- ## Probabilities step 1: Prior Probabilities `P(Class)`  -->\n",
    "\n",
    "`P(Class)` allows us to say for each class its probability. Specifically this means what is the frequency of this class in the dataset. While we write it here as the general variable `Class`, when we write it for a specific instance we write it as `P(Class=\"spam\")`, or if it is clear for which variable the instance is we can just say `P(\"spam\")`.\n",
    "- To compute the prior probability of the \"spam\" class, we count the number of spam documents and divide by the total number of documents:\n",
    "\n",
    "$$\n",
    "P(\\text{Class} = \\text{spam}) = \\frac{\\# \\text{Spam documents}}{\\#\\text{Spam documents} + \\# \\text{Non spam documents}}\n",
    "$$\n",
    "- Notation remark: Sometimes we can use \"#\" as an abbreviation for \"Number of\", and as this reads easier (and I am lazy), I write it us such.\n",
    "\n",
    " <!-- <details open> -->\n",
    " <details>\n",
    "   <summary><b>Remark</b>: Calculating P(Document) - issues </summary>\n",
    "\n",
    "**Computing `P(Document)`**\n",
    "\n",
    "For our Naive Bayes Classifier we don't need the prior pobability of a Document but it is nevertheless good to note what it would mean. \n",
    "So for the BOW model the probability of a document would be the combined probability of the individual words.\n",
    "This computes P(Document) based purely on the overall word distribution in the dataset, without considering class labels.\n",
    "\n",
    "$$\n",
    "P(\\text{word}_i) = \\frac{\\# \\text{word}_i \\text{ across all documents}}{\\# \\text{ word instances (so not just unique) across all documents}}\n",
    "$$\n",
    "\n",
    "Then, assuming independence between words (the Naive Bayes assumption), the probability of a document is the product of the probabilities of its words:\n",
    "$$\n",
    "P(\\text{Document}) = \\prod_{i=1}^{n} P(\\text{word}_i)^{c_i}\n",
    "$$\n",
    "\n",
    "where $c_i$ is the count of word $i$ in the document, and $n$ is the size of our vocabulary.\n",
    "\n",
    "**Note:** In practice, this probability will be extremely small, so we often work with log probabilities instead.\n",
    "\n",
    "$$\n",
    "log (P(\\text{Document}) )= \\sum_{i=1}^{n} log(P(\\text{word}_i)^{c_i})\n",
    "$$\n",
    "\n",
    " </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior probability of spam: P(spam) = 0.500\n",
      "Prior probability of not spam: P(not spam) = 0.500\n"
     ]
    }
   ],
   "source": [
    "labels_bin = [int(item['label'] == \"spam\") for item in dataset]  # 1 for spam, 0 for not_spam\n",
    "prob_spam = sum(labels_bin) / len(labels)\n",
    "prob_not_spam = 1 - prob_spam\n",
    "\n",
    "print(f\"\\nPrior probability of spam: P(spam) = {prob_spam:.3f}\")\n",
    "print(f\"Prior probability of not spam: P(not spam) = {prob_not_spam:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** In this case we see that the dataset is well balanced, which is great! But as you can imagine for many real tasks we will work with very imbalanced datasets. In such imbalanced cases the impact of Naive Bayes really shows it's imporance (remember the manual example in section 3). Interestingly, I just found out that our 50/50 class split is not as unrealistic, as [this source ](https://www.statista.com/statistics/420400/spam-email-traffic-share-annual/)mentions spam is about 45% of all email traffic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Computing Likelihood Probabilities\n",
    "\n",
    "Here's where the \"naive\" part comes in. We assume that **all words in a document are independent** given the class. Of course in the real world this is not true, for example if a review contains a negative word like \"awful\" it is much more likely that other negative words will be in that review than positive words. However, like all things in science, this method offers approximation of the truth (the probability of a class in this case), which it turns out is very useful so we keep using it, but do be aware of this limitation.\n",
    "\n",
    "For a document with words w₁, w₂, ..., wₙ:\n",
    "\n",
    "```\n",
    "P(Document | Class) = P(w₁ | Class) × P(w₂ | Class) × ... × P(wₙ | Class)\n",
    "```\n",
    "\n",
    "This probability function is a Conditional Probability, as we are computing: what is the probability of having this document, knowing that it belongs to class X (e.g. spam). So if this document looks nothing like other spam documents its probability will be very low. \n",
    "\n",
    "For our case we can just count how often each word in our vocabulary appears in each of the class, then we can calculate the probability of w1 given class=\"spam\", by counting how often w1 appears in all the spam documents, and normalizing (dividing) by the total words in that class.\n",
    "\n",
    "\n",
    "<!-- $$\n",
    " P(\\text{word}_i | Class=\\text{\"spam\"}) = \\frac{\\text{\\# Occurences word}_i \\text{ in all spam documents}}{\\text{\\# Occurences of word instances (so not just unique) in all spam documents}}\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "P(\\text{word}_i\\,|\\,\\text{Class=spam}) = \\frac{\\#\\text{times word}_i \\text{ in all spam documents}}{\\#\\text{total words in all spam documents}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practically with Naive Bayes we count the frequencies of each word\n",
    "Practical training steps:\n",
    "1. **Count word frequencies** in each class\n",
    "2. **Calculate probabilities**:\n",
    "   - P(Class): How common is each class?\n",
    "   - P(word | Class): How often does each word appear in each class?\n",
    "3. **Apply smoothing** to handle unseen words\n",
    "\n",
    "\n",
    "For Prediction:\n",
    "1. **For each class**, calculate: P(Class) × ∏ P(word | Class) for all words\n",
    "2. **Choose the class** with the highest probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100, 2)\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "spam        50\n",
      "not_spam    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few examples:\n",
      "                                                text     label\n",
      "0  Looking for quick cash? Look no further! Join ...      spam\n",
      "1  Our DaaS platform Quandl is a free and open in...  not_spam\n",
      "2  Should I worry about having a good IQ to becom...  not_spam\n",
      "3  Hi all,\\nworking on a research project to iden...  not_spam\n",
      "4  Does anyone know the sources for raw data?\\n\\n...  not_spam\n"
     ]
    }
   ],
   "source": [
    "train_data = list(zip(text_data, labels))\n",
    "train_df = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nFirst few examples:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 2073\n",
      "First 20 words in vocabulary: ['\"', '\"buy', '\"dataisbeautiful\"', '\"friends\"', '\"get', '\"hey', '\"hoarding\",', '\"howdy', '\"inappropriate', '\"lose', '\"spam', '\"upgrade', '\"urgent!', '\"want', '#', '##', '#baliadventure', '#beachbody', '#fitness', '#followers4life']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenization: lowercase and split by spaces\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = set()\n",
    "for text, _ in train_data:\n",
    "    vocabulary.update(tokenize(text))\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
    "print(f\"First 20 words in vocabulary: {sorted(list(vocabulary))[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Count Word Frequencies per Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "  spam: 50 documents\n",
      "  not_spam: 50 documents\n",
      "\n",
      "Word counts per class:\n",
      "\n",
      "spam:\n",
      "  'and': 120\n",
      "  'you': 91\n",
      "  'to': 73\n",
      "  'our': 70\n",
      "  'the': 67\n",
      "\n",
      "not_spam:\n",
      "  'to': 95\n",
      "  'the': 94\n",
      "  'i': 91\n",
      "  'a': 86\n",
      "  'of': 78\n"
     ]
    }
   ],
   "source": [
    "# Count documents per class\n",
    "class_counts = Counter([label for _, label in train_data])\n",
    "total_docs = len(train_data)\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"  {cls}: {count} documents\")\n",
    "\n",
    "# Count word frequencies per class\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "total_words = defaultdict(int)\n",
    "\n",
    "for text, label in train_data:\n",
    "    words = tokenize(text)\n",
    "    for word in words:\n",
    "        word_counts[label][word] += 1\n",
    "        total_words[label] += 1\n",
    "\n",
    "print(\"\\nWord counts per class:\")\n",
    "for label in ['spam', 'not_spam']:\n",
    "    print(f\"\\n{label}:\")\n",
    "    # Show only top 5 most common words per class (since vocabulary is much larger)\n",
    "    label_word_counts = word_counts[label]\n",
    "    top_words = sorted(label_word_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, count in top_words:\n",
    "        print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Calculate Probabilities\n",
    "**Prior probabilities** (P(Class)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam) = 50/100 = 0.500\n",
      "P(not_spam) = 50/100 = 0.500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prior probabilities\n",
    "priors = {}\n",
    "for cls, count in class_counts.items():\n",
    "    priors[cls] = count / total_docs\n",
    "    print(f\"P({cls}) = {count}/{total_docs} = {priors[cls]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Likelihood probabilities ($P(\\text{word} \\mid \\text{Class})$) -  **Smoothing**\n",
    "\n",
    "The basic likelihood probability $P(\\text{word} \\mid \\text{Class})$, which provides the likelihood of word $\\text{word}$ in a document given that we are in class $\\text{Class}$, is:\n",
    "\n",
    "$$\n",
    "P(\\text{word} \\mid \\text{class}) = \\frac{\\text{count}(\\text{word}, \\text{class})}{\\text{total words in class}}\n",
    "$$\n",
    "\n",
    "However, since a word can also appear $0$ times for a specific class, to avoid zero probabilities for unseen words, we use **Laplace smoothing** (add-one smoothing):\n",
    "\n",
    "$$\n",
    "P(\\text{word} \\mid \\text{class}) = \\frac{\\text{count}(\\text{word}, \\text{class}) + \\alpha}{\\text{total words in class} + \\text{vocabulary size}*\\alpha}\n",
    "$$\n",
    "\n",
    "For our case we use $\\alpha = 1$ for simplicity (though keep in mind that varying this parameter might be relevant in other cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Likelihood probabilities P(word | class):\n",
      "\n",
      "Vocabulary size: 2073\n",
      "\n",
      "Total words in spam: 2909\n",
      "Examples for spam:\n",
      "  P('free' | spam) = (8 + 1) / (2909 + 2073) = 0.001807\n",
      "  P('call' | spam) = (0 + 1) / (2909 + 2073) = 0.000201\n",
      "  P('win' | spam) = (8 + 1) / (2909 + 2073) = 0.001807\n",
      "  P('click' | spam) = (5 + 1) / (2909 + 2073) = 0.001204\n",
      "  P('urgent' | spam) = (1 + 1) / (2909 + 2073) = 0.000401\n",
      "\n",
      "Total words in not_spam: 3339\n",
      "Examples for not_spam:\n",
      "  P('free' | not_spam) = (5 + 1) / (3339 + 2073) = 0.001109\n",
      "  P('call' | not_spam) = (0 + 1) / (3339 + 2073) = 0.000185\n",
      "  P('win' | not_spam) = (0 + 1) / (3339 + 2073) = 0.000185\n",
      "  P('click' | not_spam) = (0 + 1) / (3339 + 2073) = 0.000185\n",
      "  P('urgent' | not_spam) = (0 + 1) / (3339 + 2073) = 0.000185\n"
     ]
    }
   ],
   "source": [
    "# Likelihood probabilities with Laplace smoothing\n",
    "alpha = 1  # Smoothing parameter\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "def calculate_likelihood(word, cls):\n",
    "    \"\"\"Calculate P(word | class) with Laplace smoothing\"\"\"\n",
    "    word_count = word_counts[cls][word]\n",
    "    total = total_words[cls]\n",
    "    return (word_count + alpha) / (total + alpha * vocab_size)\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nLikelihood probabilities P(word | class):\")\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "\n",
    "# Pick some characteristic spam/not_spam words for examples\n",
    "example_words = ['free', 'call', 'win', 'click', 'urgent']\n",
    "\n",
    "for label in ['spam', 'not_spam']:\n",
    "    print(f\"\\nTotal words in {label}: {total_words[label]}\")\n",
    "    print(f\"Examples for {label}:\")\n",
    "    for word in example_words:\n",
    "        prob = calculate_likelihood(word, label)\n",
    "        count = word_counts[label][word]\n",
    "        print(f\"  P('{word}' | {label}) = ({count} + 1) / ({total_words[label]} + {vocab_size}) = {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Making Predictions\n",
    "\n",
    "<!-- ### Step 4: Make Predictions -->\n",
    "\n",
    "Now let's classify some new emails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Message: 'Congratulations! You won a free prize. Click here now!'\n",
      "Log probabilities:\n",
      "  spam: -61.54\n",
      "  not_spam: -67.06\n",
      "Predicted: spam\n",
      "\n",
      "Message: 'Hi, are we still meeting for lunch tomorrow?'\n",
      "Log probabilities:\n",
      "  spam: -59.18\n",
      "  not_spam: -58.89\n",
      "Predicted: not_spam\n",
      "\n",
      "Message: 'URGENT: Your account will be closed. Call immediately!'\n",
      "Log probabilities:\n",
      "  spam: -59.67\n",
      "  not_spam: -64.41\n",
      "Predicted: spam\n",
      "\n",
      "Message: 'Thanks for the document you sent yesterday'\n",
      "Log probabilities:\n",
      "  spam: -47.91\n",
      "  not_spam: -47.01\n",
      "Predicted: not_spam\n"
     ]
    }
   ],
   "source": [
    "def predict_naive_bayes(text):\n",
    "    \"\"\"Predict the class for a text message\"\"\"\n",
    "    words = tokenize(text)\n",
    "    \n",
    "    scores = {}\n",
    "    for cls in ['spam', 'not_spam']:\n",
    "        # Start with prior probability (in log space to avoid underflow)\n",
    "        score = np.log(priors[cls])\n",
    "        \n",
    "        # Multiply by likelihood of each word (add in log space)\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                score += np.log(calculate_likelihood(word, cls))\n",
    "            else:\n",
    "                # Unseen word: use smoothing\n",
    "                score += np.log(alpha / (total_words[cls] + alpha * vocab_size))\n",
    "        \n",
    "        scores[cls] = score\n",
    "    \n",
    "    # Return class with highest score\n",
    "    predicted_class = max(scores, key=scores.get)\n",
    "    return predicted_class, scores\n",
    "\n",
    "# Test on new messages\n",
    "test_messages = [\n",
    "    \"Congratulations! You won a free prize. Click here now!\",\n",
    "    \"Hi, are we still meeting for lunch tomorrow?\",\n",
    "    \"URGENT: Your account will be closed. Call immediately!\",\n",
    "    \"Thanks for the document you sent yesterday\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for message in test_messages:\n",
    "    predicted_class, scores = predict_naive_bayes(message)\n",
    "    print(f\"\\nMessage: '{message}'\")\n",
    "    print(f\"Log probabilities:\")\n",
    "    for cls, score in scores.items():\n",
    "        print(f\"  {cls}: {score:.2f}\")\n",
    "    print(f\"Predicted: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<!-- ## 5. Implementation with scikit-learn -->\n",
    "### 3.5 Using existing package: scikit-learn\n",
    "While building our own Naive Bayes classifier from scratch is valuable for understanding, in practice we use well-tested libraries like **scikit-learn**. Let's see how the professional version compares to our handmade one.\n",
    "\n",
    "The scikit-learn implementation involves three components working together:\n",
    "\n",
    "**CountVectorizer** handles all the messy text preprocessing for us. It takes raw text and converts it into a matrix of word counts—exactly the Bag-of-Words representation we built manually. When you call `fit()`, it builds a vocabulary from your training data, then `transform()` converts any text into counts using that vocabulary. It returns a sparse matrix, which is important: if your vocabulary has 10,000 words but a document only contains 50 unique words, why store 9,950 zeros?\n",
    "\n",
    "**MultinomialNB** is scikit-learn's Naive Bayes for count data. Under the hood, it's doing exactly what we coded earlier: calculating P(Class) and P(word|Class) for every word in the vocabulary, with Laplace smoothing (controlled by the `alpha` parameter, defaulting to 1.0). The \"Multinomial\" refers to the statistical distribution it assumes for word counts.\n",
    "\n",
    "**Pipeline** chains these steps together elegantly. Think of it as a recipe: \"First vectorize the text, then classify it.\" This has a subtle but important benefit—when you call `fit()` on the pipeline, it learns the vocabulary from training data only. When you call `predict()`, it uses that same vocabulary, ensuring train-test consistency. Without a pipeline, you might accidentally introduce data leakage by letting test data influence your vocabulary.\n",
    "\n",
    "Here's how it all works together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCIKIT-LEARN PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Message: 'Congratulations! You won a free prize. Click here now!'\n",
      "Probabilities: not_spam=0.000, spam=1.000\n",
      "\n",
      "Message: 'Hi, are we still meeting for lunch tomorrow?'\n",
      "Probabilities: not_spam=0.860, spam=0.140\n",
      "\n",
      "Message: 'URGENT: Your account will be closed. Call immediately!'\n",
      "Probabilities: not_spam=0.002, spam=0.998\n",
      "\n",
      "Message: 'Thanks for the document you sent yesterday'\n",
      "Probabilities: not_spam=0.817, spam=0.183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare data\n",
    "X_train = [text for text, _ in train_data]\n",
    "y_train = [label for _, label in train_data]\n",
    "\n",
    "# Create a pipeline\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the same test messages\n",
    "test_messages_sklearn = [\n",
    "    \"Congratulations! You won a free prize. Click here now!\",\n",
    "    \"Hi, are we still meeting for lunch tomorrow?\",\n",
    "    \"URGENT: Your account will be closed. Call immediately!\",\n",
    "    \"Thanks for the document you sent yesterday\"\n",
    "]\n",
    "\n",
    "predictions = model.predict(test_messages_sklearn)\n",
    "probabilities = model.predict_proba(test_messages_sklearn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCIKIT-LEARN PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for message, pred, probs in zip(test_messages_sklearn, predictions, probabilities):\n",
    "    print(f\"\\nMessage: '{message}'\")\n",
    "    # Note: class order is alphabetically sorted in sklearn\n",
    "    class_labels = model.classes_\n",
    "    prob_str = ', '.join([f\"{label}={prob:.3f}\" for label, prob in zip(class_labels, probs)])\n",
    "    print(f\"Probabilities: {prob_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of examples after selection: 2725\n"
     ]
    }
   ],
   "source": [
    "#  Load the test set\n",
    "full_dataset_test = load_dataset('Deysi/spam-detection-dataset', split='test')  # Load only first 100 examples for speed\n",
    "\n",
    "full_dataset_test = full_dataset_test.shuffle(seed=42)\n",
    "# dataset_test = full_dataset_test.select(range(100)) # If you want use a subset for speed\n",
    "dataset_test = full_dataset_test\n",
    "print(f\"\\nNumber of examples after selection: {len(dataset_test)}\")\n",
    "\n",
    "labels_test = [item['label'] == \"spam\" for item in dataset_test]  # 1 for spam, 0 for not_spam\n",
    "text_data_test = [item['text'] for item in dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET EVALUATION\n",
      "============================================================\n",
      "Test Accuracy: 99.41% (2709/2725 correct)\n"
     ]
    }
   ],
   "source": [
    "def calculate_test_accuracy(model, text_data_test, labels_test):\n",
    "\n",
    "    # Convert boolean labels to string labels if needed\n",
    "    y_test = ['spam' if label else 'not_spam' for label in labels_test]\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(text_data_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum(pred == true for pred, true in zip(y_pred, y_test))\n",
    "    accuracy = correct / len(y_test)\n",
    "    \n",
    "    return accuracy, y_pred, y_test\n",
    "\n",
    "# Calculate test accuracy\n",
    "accuracy, y_pred, y_test = calculate_test_accuracy(model, text_data_test, labels_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {accuracy:.2%} ({int(accuracy * len(y_test))}/{len(y_test)} correct)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "- Wow! we see that for this dataset the model already obtains nearly perfect accuracy on the test set (2709 example), while the NB classifier is only trained on 100 example.\n",
    "- Likely this dataset is fairly simple though. Potential follow-up experiments: 1. Which words are most indicative of a specific class? If we only evaluated 10 or a 100 words in our vocabulator, what would the test accuracy be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4. Extra Material\n",
    "If you walked through all the above material and feel comfortable implementing similar systems your self this is already great! Below are some extra code samples to help you get familiar with additional tasks and other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Another classification task: Sentiment Analysis\n",
    "While the spam dataset we used was a nice toy example, it is not a very popular dataset, nor very challenging if our Naive Bayes classifier already has 100% accuracy when trained on only 100 examples. \n",
    "\n",
    "Let's try with more data to see realistic performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMDB SENTIMENT CLASSIFICATION\n",
      "============================================================\n",
      "Training samples: 25000\n",
      "Test samples: 25000\n",
      "Test Accuracy: 82.34%\n",
      "\n",
      "### Example predictions:\n",
      "\n",
      "Review: 'This movie was absolutely fantastic! Great acting and story....'\n",
      "Predicted: positive (confidence: 78.95%)\n",
      "\n",
      "Review: 'Terrible film, waste of time and money....'\n",
      "Predicted: negative (confidence: 99.21%)\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB dataset and train Naive Bayes classifier\n",
    "imdb_dataset = load_dataset('imdb', split='train')  # Use 5000 samples for speed\n",
    "imdb_test = load_dataset('imdb', split='test')  # Use 1000 test samples\n",
    "\n",
    "# Prepare data\n",
    "X_train_imdb = imdb_dataset['text']\n",
    "y_train_imdb = ['positive' if label == 1 else 'negative' for label in imdb_dataset['label']]\n",
    "X_test_imdb = imdb_test['text']\n",
    "y_test_imdb = ['positive' if label == 1 else 'negative' for label in imdb_test['label']]\n",
    "\n",
    "# Train model\n",
    "mn_nb_model_imdb = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=5000)),  # Limit features for speed\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "mn_nb_model_imdb.fit(X_train_imdb, y_train_imdb)\n",
    "\n",
    "# Calculate test accuracy\n",
    "y_pred_imdb_mn = mn_nb_model_imdb.predict(X_test_imdb)\n",
    "accuracy_imdb_mn = sum(pred == true for pred, true in zip(y_pred_imdb_mn, y_test_imdb)) / len(y_test_imdb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMDB SENTIMENT CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(X_train_imdb)}\")\n",
    "print(f\"Test samples: {len(X_test_imdb)}\")\n",
    "print(f\"Test Accuracy: {accuracy_imdb_mn:.2%}\")\n",
    "\n",
    "# Show example predictions\n",
    "test_examples = [\n",
    "    \"This movie was absolutely fantastic! Great acting and story.\",\n",
    "    \"Terrible film, waste of time and money.\"\n",
    "]\n",
    "\n",
    "print(\"\\n### Example predictions:\")\n",
    "for text in test_examples:\n",
    "    pred = mn_nb_model_imdb.predict([text])[0]\n",
    "    probs = mn_nb_model_imdb.predict_proba([text])[0]\n",
    "    print(f\"\\nReview: '{text[:60]}...'\")\n",
    "    print(f\"Predicted: {pred} (confidence: {max(probs):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2. Variants of Naive Bayes\n",
    "\n",
    "There are different variants depending on the type of features:\n",
    "\n",
    "#### MultinomialNB (What we used)\n",
    "- For **count data** (word frequencies)\n",
    "- Best for text classification\n",
    "- Assumes features follow a multinomial distribution\n",
    "\n",
    "#### BernoulliNB\n",
    "- For **binary features** (word present/absent)\n",
    "- Can work well with short documents\n",
    "- Assumes features are binary\n",
    "\n",
    "#### GaussianNB\n",
    "- For **continuous features**\n",
    "- Assumes features follow a Gaussian (normal) distribution\n",
    "- Not typically used for text , very compute heavy as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy Comparison:\n",
      "MultinomialNB: 82.34%\n",
      "BernoulliNB  : 82.60%\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB\n",
    "bernoulli_model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=True)),\n",
    "    ('classifier', BernoulliNB(alpha=1.0))\n",
    "])\n",
    "bernoulli_model.fit(X_train_imdb, y_train_imdb)\n",
    "bernoulli_pred = bernoulli_model.predict(X_test_imdb)\n",
    "accuracy_bernoulli = np.mean(bernoulli_pred == y_test_imdb)\n",
    "\n",
    "\n",
    "print(\"\\nTest Accuracy Comparison:\")\n",
    "print(f\"MultinomialNB: {accuracy_imdb_mn:.2%}\")\n",
    "print(f\"BernoulliNB  : {accuracy_bernoulli:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. (optional) Exercise \n",
    "\n",
    "**Task**: Use the Naive Bayes classifier to build a language identifier!\n",
    "\n",
    "Given the following training data of sentences in different languages, train a classifier to identify the language of new sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LANGUAGE IDENTIFICATION\n",
      "============================================================\n",
      "'hello world' → english\n",
      "'buenos amigos' → spanish\n",
      "'bonjour monde' → french\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training data: (sentence, language)\n",
    "language_data = [\n",
    "    (\"hello how are you\", \"english\"),\n",
    "    (\"good morning everyone\", \"english\"),\n",
    "    (\"bonjour comment allez vous\", \"french\"),\n",
    "    (\"bonne journée à tous\", \"french\"),\n",
    "    (\"hola cómo estás\", \"spanish\"),\n",
    "    (\"buenos días a todos\", \"spanish\"),\n",
    "]\n",
    "\n",
    "# Your code here:\n",
    "# 1. Split into X_train and y_train\n",
    "# 2. Create and train a Naive Bayes classifier\n",
    "# 3. Test on new sentences: \"hello world\", \"buenos amigos\", \"bonjour monde\"\n",
    "\n",
    "# Solution (try yourself first!)\n",
    "X_lang = [sent for sent, _ in language_data]\n",
    "y_lang = [lang for _, lang in language_data]\n",
    "\n",
    "lang_model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer='char', ngram_range=(2, 3))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "lang_model.fit(X_lang, y_lang)\n",
    "\n",
    "test_sentences = [\"hello world\", \"buenos amigos\", \"bonjour monde\"]\n",
    "lang_predictions = lang_model.predict(test_sentences)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LANGUAGE IDENTIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sentence, lang in zip(test_sentences, lang_predictions):\n",
    "    print(f\"'{sentence}' → {lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Self-Check Questions\n",
    "\n",
    "Test your understanding of Naive Bayes and its application in this notebook:\n",
    "\n",
    "1.  What does the \"naive\" assumption in Naive Bayes refer to? Why is it important for the algorithm? \n",
    "2.  How do you compute the prior probability $P(\\text{Class})$ from a labeled dataset? \n",
    "3.  Explain how Laplace (add-one) smoothing helps when calculating $P(\\text{word} \\mid \\text{Class})$. \n",
    "4.  Given a new email, describe the steps Naive Bayes uses to predict whether it is spam or not. \n",
    "5.  Why do we often use log probabilities instead of multiplying raw probabilities directly? \n",
    "6.  What are some limitations of Naive Bayes for text classification? \n",
    "7.  How does the Bag-of-Words representation affect the information available to the classifier? \n",
    "8.  How does the vocabulary size influence the likelihood calculations and smoothing? \n",
    "9.  If a word in a test document was never seen in training, how does Naive Bayes handle it? \n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Scikit-learn Naive Bayes Documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- Jurafsky & Martin, \"Speech and Language Processing\" (Chapter on Text Classification)\n",
    "- Manning et al., \"Introduction to Information Retrieval\" (Chapter 13)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
